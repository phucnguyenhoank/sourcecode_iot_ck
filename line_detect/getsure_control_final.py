import cv2
import mediapipe as mp
import socket
from collections import deque
from statistics import mode
import time

# Th√¥ng tin ESP32 (thay ƒë·ªïi n·∫øu c·∫ßn)
ESP32_IP = "192.168.197.29"
ESP32_PORT = 5000
STACK_SIZE = 10  # K√≠ch th∆∞·ªõc stack
ACTION_MAP = {1: 'forward', 2: 'left', 3: 'right', 4: 'reverse', 0: 'stop'}
SEND_INTERVAL = 0.5  # G·ª≠i l·ªánh m·ªói 500ms

class GestureController:
    def __init__(self):
        # Kh·ªüi t·∫°o Mediapipe Hands
        self.mp_hands = mp.solutions.hands.Hands(
            model_complexity=0,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.mp_draw = mp.solutions.drawing_utils
        self.cap = cv2.VideoCapture(0)  # M·ªü webcam
        self.detect_stack = deque(maxlen=STACK_SIZE)  # Stack l∆∞u detect
        self.current_action = None  # H√†nh ƒë·ªông hi·ªán t·∫°i
        self.last_send_time = 0  # Th·ªùi gian g·ª≠i l·ªánh cu·ªëi

    def send_udp_command(self, action):
        """G·ª≠i l·ªánh UDP t·ªõi ESP32"""
        if action == 'stop':
            print("[üõë] D·ª´ng xe (kh√¥ng g·ª≠i l·ªánh)")
            return
        cmd = f"{action},200,200\n"
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as sock:
            sock.sendto(cmd.encode('utf-8'), (ESP32_IP, ESP32_PORT))
        print(f"[üì§] G·ª≠i l·ªánh: {cmd.strip()}")

    def count_fingers(self, hand_landmarks, img_shape):
        """ƒê·∫øm s·ªë ng√≥n tay"""
        myHand = [[int(lm.x * img_shape[1]), int(lm.y * img_shape[0])] for lm in hand_landmarks.landmark]
        count = 0
        # ƒê·∫øm ng√≥n tr·ªè, gi·ªØa, nh·∫´n, √∫t
        for idx in [8, 12, 16, 20]:
            if myHand[idx][1] < myHand[idx - 2][1]:
                count += 1
        # ƒê·∫øm ng√≥n c√°i
        if (myHand[4][0] < myHand[2][0] and myHand[5][0] <= myHand[13][0]) or \
           (myHand[4][0] > myHand[2][0] and myHand[5][0] >= myHand[13][0]):
            count += 1
        return count

    def run(self):
        """Ch·∫°y v√≤ng l·∫∑p ch√≠nh"""
        while self.cap.isOpened():
            success, img = self.cap.read()
            if not success:
                break

            # X·ª≠ l√Ω ·∫£nh
            img = cv2.flip(img, 1)
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            result = self.mp_hands.process(img_rgb)
            img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)

            total_fingers = 0
            # Ph√°t hi·ªán tay v√† ƒë·∫øm ng√≥n
            if result.multi_hand_landmarks:
                for hand in result.multi_hand_landmarks:
                    self.mp_draw.draw_landmarks(img, hand, mp.solutions.hands.HAND_CONNECTIONS)
                    total_fingers += self.count_fingers(hand, img.shape)

            # Th√™m s·ªë ng√≥n tay v√†o stack
            self.detect_stack.append(total_fingers)

            # Khi stack ƒë·ªß k√≠ch th∆∞·ªõc, t√≠nh mode ƒë·ªÉ c·∫≠p nh·∫≠t h√†nh ƒë·ªông
            if len(self.detect_stack) == STACK_SIZE:
                try:
                    finger_mode = mode(self.detect_stack)
                    if finger_mode in ACTION_MAP:
                        self.current_action = ACTION_MAP[finger_mode]
                except:
                    pass  # B·ªè qua n·∫øu kh√¥ng c√≥ mode duy nh·∫•t

            # G·ª≠i l·ªánh li√™n t·ª•c n·∫øu kh√¥ng ph·∫£i 'stop'
            current_time = time.time()
            if self.current_action and self.current_action != 'stop' and (current_time - self.last_send_time >= SEND_INTERVAL):
                self.send_udp_command(self.current_action)
                self.last_send_time = current_time

            # Hi·ªÉn th·ªã th√¥ng tin
            cv2.putText(img, f"Fingers: {total_fingers}", (10, 60),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
            if self.current_action:
                cv2.putText(img, f"Action: {self.current_action}", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            cv2.imshow("üñêÔ∏è ƒêi·ªÅu khi·ªÉn c·ª≠ ch·ªâ tay", img)

            # Tho√°t khi nh·∫•n 'q'
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        self.cap.release()
        cv2.destroyAllWindows()
        print("[üõë] ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh.")

def main():
    controller = GestureController()
    controller.run()

if __name__ == "__main__":
    main()